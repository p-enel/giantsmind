# from langchain.vectorstores import Chroma
# from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
# from langchain.docstore.document import Document
# from langchain.text_splitter import RecursiveCharacterTextSplitter

# # Initialize the embedding model (OpenAI in this example, you can use others)
# MODELS = {"bge-small": {"model": "BAAI/bge-base-en-v1.5", "vector_size": 768}}
# embedding = FastEmbedEmbeddings(model_name=MODELS["bge-small"]["model"])

# persist_directory = "/home/pierre/.local/share/giantsmind/"

# # Initialize Chroma as the vectorstore
# # This will store the embeddings locally by default
# chroma_db = Chroma(
#     embedding_function=embedding, collection_name="my_collection", persist_directory=persist_directory
# )

# # Add some documents to the ChromaDB for storage
# texts = [
#     "Langchain is a framework for developing language model applications.",
#     "ChromaDB is a fast and efficient vector store that can be used with Langchain.",
#     "OpenAI's GPT-3 is a powerful model for generating text and understanding queries.",
#     "Retrieval-based question answering systems use vector databases to store and retrieve data.",
# ]

# text = "sdgfdg"

# long_doc = Document(page_content=text, metadata={"source": "Langchain Documentation"})

# # Split the document into smaller chunks using a text splitter
# # RecursiveCharacterTextSplitter handles splitting intelligently by breaking on sentences, paragraphs, etc.
# splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)
# chunks = splitter.split_text(long_doc.page_content)
# chunked_docs = [Document(page_content=chunk, metadata=long_doc.metadata) for chunk in chunks]


# doc = Document(page_content=doc)
# chroma_db.add_documents([doc])

# # Add documents (texts) to ChromaDB
# chroma_db.add_texts(texts)

# # Now perform a similarity search using ChromaDB
# # This will search for documents similar to the query
# query = "What is ChromaDB?"
# query = "Proximal Policy Optimization"
# vector = chroma_db.embeddings.embed_query(query)
# results = chroma_db.similarity_search_with_score(query)
# results = chroma_db.similarity_search_by_vector_with_relevance_scores(vector)

# # Output the results
# print("Results for the query '{}':".format(query))
# for i, doc in enumerate(results):
#     print(f"Document {i + 1}: {doc.page_content}")


# chroma_db.get()
# chroma_db.delete_collection()

# chroma_client = chroma_db._client

# collection = chroma_client.get_collection("my_collection")

from chromadb import chromadb
from typing import List
from langchain.vectorstores import Chroma
from langchain_core.documents.base import Document
from langchain_core.embeddings import Embeddings
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from giantsmind.vector_db.base import VectorDBClient


MODELS = {"bge-small": {"model": "BAAI/bge-base-en-v1.5", "vector_size": 768}}


def _init_fastembed(model_name: str) -> Embeddings:
    return


def _init_client(collection: str, embedding: Embeddings) -> Chroma:
    return Chroma(
        embedding_function=embedding,
        collection_name=collection,
        persist_directory=PERSIST_DIRECTORY,
    )


def _check_ids_exist(chroma_client: Chroma, IDs: List[str]) -> List[bool]:
    results = chroma_client.get(where={"paper_id": {"$in": IDs}})
    IDs_res = [metadata["paper_id"] for metadata in results["metadatas"]]
    exists = [ID in IDs_res for ID in IDs]
    return exists


class ChromadbClient(Chroma, VectorDBClient):

    def check_ids_exist(self, IDs: List[str]) -> List[bool]:
        return _check_ids_exist(self.vc_client, IDs)

    def add_documents(self, documents: List[Document], **kwargs) -> List[str]:
        return self.vc_client.add_documents(documents, **kwargs)
